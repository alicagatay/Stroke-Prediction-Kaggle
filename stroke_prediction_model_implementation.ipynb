{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1882037,"sourceType":"datasetVersion","datasetId":1120859}],"dockerImageVersionId":30786,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Problem Statement\nStroke is the second leading cause of death globally, accounting for approximately 11% of all fatalities, according to the World Health Organization (WHO). Despite advancements in healthcare, predicting the likelihood of a stroke remains a challenge. The aim of this project is to forecast the probability of an individual experiencing a stroke based on a range of health parameters. Through the analysis of data pertaining to factors such as age, gender, BMI, and medical history, a machine learning model can be developed to anticipate the likelihood of a stroke in an individual. This model holds promise for early identification, facilitating timely interventions to prevent adverse outcomes.","metadata":{}},{"cell_type":"markdown","source":"# The Dataset\nThe dataset aims to predict the likelihood of stroke by analysing real-world health data from individuals, including patientsâ€™ demographics and health attributes. The goal is to provide insights to support early intervention and prevention strategies for stroke. Each row in the data provides relevant information about the patient, such as:\n1. id: unique identifier\n2. gender: \"Male\", \"Female\" or \"Other\"\n3. age: age of the patient\n4. hypertension: 0 if the patient doesn't have hypertension, 1 if the patient has hypertension\n5. heart_disease: 0 if the patient doesn't have any heart diseases, 1 if the patient has a heart disease\n6. ever_married: \"No\" or \"Yes\"\n7. work_type: \"children\", \"Govt_jov\", \"Never_worked\", \"Private\" or \"Self-employed\"\n8. Residence_type: \"Rural\" or \"Urban\"\n9. avg_glucose_level: average glucose level in blood\n10. bmi: body mass index\n11. smoking_status: \"formerly smoked\", \"never smoked\", \"smokes\" or \"Unknown\"*\n12. stroke: 1 if the patient had a stroke or 0 if not","metadata":{}},{"cell_type":"markdown","source":"# The Import Statements\nIn order to be able to work with the dataset, and then to implement the machine learning models, we need to import the necessary libraries first.","metadata":{}},{"cell_type":"code","source":"# Pandas is used for handling data structures and data manipulation, such as reading data from CSV files, managing DataFrames, and performing operations like filtering, grouping, and aggregation.\nimport pandas as pd\n\n\n# Matplotlib's pyplot module is used for creating visualisations such as line charts, bar charts, histograms, scatter plots, and more. It provides control over plot elements like titles, labels, and legends.\nimport matplotlib.pyplot as plt\n\n\n# The RandomForestClassifier from scikit-learn is used for creating a random forest model, which is an ensemble machine learning method based on decision trees, commonly used for classification tasks.\nfrom sklearn.ensemble import RandomForestClassifier\n\n# The MLPClassifier (Multilayer Perceptron) is a neural network model that supports multi-layer architecture for performing classification tasks, especially useful for more complex datasets.\nfrom sklearn.neural_network import MLPClassifier\n\n# KNeighborsClassifier is used for implementing the k-Nearest Neighbors algorithm, which is a simple, instance-based learning algorithm for classification tasks based on feature similarity.\nfrom sklearn.neighbors import KNeighborsClassifier\n\n# Confusion matrix is a performance evaluation metric used to summarize the results of a classification model by showing the count of true positive, true negative, false positive, and false negative predictions.\nfrom sklearn.metrics import confusion_matrix\n\n# Classification report is used to provide a detailed performance evaluation of a classifier, including precision, recall, F1 score, and accuracy for each class in the dataset.\nfrom sklearn.metrics import classification_report\n\n# ConfusionMatrixDisplay is used to visually represent the confusion matrix in the form of a heatmap, making it easier to interpret the classification results.\nfrom sklearn.metrics import ConfusionMatrixDisplay\n\n# SMOTE (Synthetic Minority Oversampling Technique) is used to handle imbalanced datasets by generating synthetic examples for the minority class, improving model performance when dealing with imbalanced data.\nfrom imblearn.over_sampling import SMOTE\n\n# train_test_split is used to split the dataset into training and testing subsets. This ensures that the model is trained on one part of the data and tested on another, allowing for evaluation of model performance on unseen data.\nfrom sklearn.model_selection import train_test_split","metadata":{"execution":{"iopub.status.busy":"2024-10-18T09:34:19.794529Z","iopub.execute_input":"2024-10-18T09:34:19.795130Z","iopub.status.idle":"2024-10-18T09:34:21.524123Z","shell.execute_reply.started":"2024-10-18T09:34:19.795064Z","shell.execute_reply":"2024-10-18T09:34:21.523171Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-18T09:34:34.620216Z","iopub.execute_input":"2024-10-18T09:34:34.620996Z","iopub.status.idle":"2024-10-18T09:34:34.639373Z","shell.execute_reply.started":"2024-10-18T09:34:34.620943Z","shell.execute_reply":"2024-10-18T09:34:34.637822Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"/kaggle/input/stroke-prediction-dataset/healthcare-dataset-stroke-data.csv\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Creating the Machine Learning Model Class\nSince we will implement multiple machine learning models and compare their performances, and also because we want to comply with the rules and design of Object-Oriented Programming principles, we will create a class called `StrokeRiskPredictor` that will contain all the machine learning models, their implementations, and data preprocessing steps as different functions.","metadata":{}},{"cell_type":"code","source":"class StrokeRiskPredictor:\n    def __init__(self, data_path):\n        # Load the dataset from the specified path\n        self.data = pd.read_csv(data_path)","metadata":{"execution":{"iopub.status.busy":"2024-10-18T09:37:24.069519Z","iopub.execute_input":"2024-10-18T09:37:24.070079Z","iopub.status.idle":"2024-10-18T09:37:24.077676Z","shell.execute_reply.started":"2024-10-18T09:37:24.070032Z","shell.execute_reply":"2024-10-18T09:37:24.075891Z"},"trusted":true},"execution_count":3,"outputs":[]}]}